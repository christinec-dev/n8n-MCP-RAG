{
  "source": "1804_Stickynote_Automation_Triggered.json",
  "index": 1,
  "content": "{\n  \"id\": \"af8RV5b2TWB2LclA\",\n  \"meta\": {\n    \"instanceId\": \"95f2ab28b3dabb8da5d47aa5145b95fe3845f47b20d6343dd5256b6a28ba8fab\",\n    \"templateCredsSetupCompleted\": true\n  },\n  \"name\": \"Chat with local LLMs using n8n and Ollama\",\n  \"tags\": [],\n  \"nodes\": [\n    {\n      \"id\": \"475385fa-28f3-45c4-bd1a-10dde79f74f2\",\n      \"name\": \"When chat message received\",\n      \"type\": \"@n8n/n8n-nodes-langchain.chatTrigger\",\n      \"position\": [\n        700,\n        460\n      ],\n      \"webhookId\": \"ebdeba3f-6b4f-49f3-ba0a-8253dd226161\",\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"typeVersion\": 1.1\n    },\n    {\n      \"id\": \"61133dc6-dcd9-44ff-85f2-5d8cc2ce813e\",\n      \"name\": \"Ollama Chat Model\",\n      \"type\": \"@n8n/n8n-nodes-langchain.lmChatOllama\",\n      \"position\": [\n        900,\n        680\n      ],\n      \"parameters\": {\n        \"options\": {}\n      },\n      \"credentials\": {\n        \"ollamaApi\": {\n          \"id\": \"MyYvr1tcNQ4e7M6l\",\n          \"name\": \"Local Ollama\"\n        }\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"3e89571f-7c87-44c6-8cfd-4903d5e1cdc5\",\n      \"name\": \"Sticky Note\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        160,\n        80\n      ],\n      \"parameters\": {\n        \"width\": 485,\n        \"height\": 473,\n        \"content\": \"## Chat with local LLMs using n8n and Ollama\\nThis n8n workflow allows you to seamlessly interact with your self-hosted Large Language Models (LLMs) through a user-friendly chat interface. By connecting to Ollama, a powerful tool for managing local LLMs, you can send prompts and receive AI-generated responses directly within n8n.\\n\\n### How it works\\n1. When chat message received: Captures the user's input from the chat interface.\\n2. Chat LLM Chain: Sends the input to the Ollama server and receives the AI-generated response.\\n3. Delivers the LLM's response back to the chat interface.\\n\\n### Set up steps\\n* Make sure Ollama is installed and running on your machine before executing this workflow.\\n* Edit the Ollama address if different from the default.\\n\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"9345cadf-a72e-4d3d-b9f0-d670744065fe\",\n      \"name\": \"Sticky Note1\",\n      \"type\": \"n8n-nodes-base.stickyNote\",\n      \"position\": [\n        1040,\n        660\n      ],\n      \"parameters\": {\n        \"color\": 6,\n        \"width\": 368,\n        \"height\": 258,\n        \"content\": \"## Ollama setup\\n* Connect to your local Ollama, usually on http://localhost:11434\\n* If running in Docker, make sure that the n8n container has access to the host's network in order to connect to Ollama. You can do this by passing `--net=host` option when starting the n8n Docker container\"\n      },\n      \"typeVersion\": 1\n    },\n    {\n      \"id\": \"eeffdd4e-6795-4ebc-84f7-87b5ac4167d9\",\n      \"name\": \"Chat LLM Chain\",\n      \"type\": \"@n8n/n8n-nodes-langchain.chainLlm\",\n      \"position\": [\n        920,\n        460\n      ],\n      \"parameters\": {},\n      \"typeVersion\": 1.4\n    }\n  ],\n  \"active\": false,\n  \"pinData\": {},\n  \"settings\": {\n    \"executionOrder\": \"v1\"\n  },\n  \"versionId\": \"3af03daa-e085-4774-8676-41578a4cba2d\",\n  \"connections\": {\n    \"Ollama Chat Model\": {\n      \"ai_languageModel\": [\n        [\n          {\n            \"node\": \"Chat LLM Chain\",\n            \"type\": \"ai_languageModel\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"When chat message received\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Chat LLM Chain\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  }\n}",
  "metadata": {
    "node_types": [
      "@n8n/n8n-nodes-langchain.chatTrigger",
      "@n8n/n8n-nodes-langchain.lmChatOllama",
      "n8n-nodes-base.stickyNote",
      "n8n-nodes-base.stickyNote",
      "@n8n/n8n-nodes-langchain.chainLlm"
    ],
    "trigger": null
  }
}