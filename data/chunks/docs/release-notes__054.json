{
  "source": "docs/release-notes.md",
  "index": 54,
  "content": "### Evaluations for AI workflows\n\nWeâ€™ve added a feature to help you iterate, test, and compare changes to your AI automations before pushing them to production so you can achieve more predictability and make better decisions.<br><br>\n\nWhen you're building with AI, a small prompt tweak or model swap might improve results with some inputs, while quietly degrading performance with others. But without a way to evaluate performance across many inputs, youâ€™re left guessing whether your AI is actually getting better when you make a change.  <br><br>\n\nBy implementing **Evaluations for AI workflows** in n8n, you can assess how your AI performs across a range of inputs by adding a dedicated path in your workflow for running test cases and applying custom metrics to track results. This helps you build viable proof-of-concepts quickly, iterate more effectively, catch regressions early, and make more confident decisions when your AI is in production.<br><br>\n\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/5LlF196PKaE?si=TcwM0JyhjsRKDb3x\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n<br><br>\n\n#### Evaluation node and tab\n\nThe **Evaluation node** includes several operations that, when used together, enable end-to-end AI evaluation.\n\n<br> \n<figure markdown=\"span\">\n    ![Evaluation node](/_images/release-notes/Evaluations_node.png)\n    <figcaption>Evaluation node</figcaption>\n</figure>\n<br>\n\nUse this node to:\n\n- Run your AI logic against a wide range of test cases in the same execution\n- Capture the outputs of those test cases\n- Score the results using your own metrics or LLM-as-judge logic\n- Isolate a testing path to only include the nodes and logic you want to evaluate <br>\n\nThe **Evaluations tab** enables you to review test results in the n8n UI, perfect for comparing runs, spotting regressions, and viewing performance over time.\n<br><br>\n\n#### ðŸ›  How evaluations work\n\nThe evaluation path runs alongside your normal execution logic and only activates when you wantâ€”making it ideal for testing and iteration. <br><br>\n\nGet started by selecting an AI workflow you want to evaluate that includes one or more LLM or Agent nodes. <br>\n\n1. Add an **Evaluation** node with the **On new Evaluation event** operation. This node will act as an additional trigger youâ€™ll run only when testing. Configure it to read your dataset from Google Sheets, with each row representing a test input.<br>\n\n    > ðŸ’¡  Better datasets mean better evaluations. Craft your dataset from a variety of test cases, including edge cases and typical inputs, to get meaningful feedback on how your AI performs. Learn more and access sample datasets [here](/advanced-ai/evaluations/light-evaluations.md/#1-create-a-dataset).\n\n2. Add a second **Evaluation** node using the **Set Outputs** operation after the part of the workflow you're testingâ€”typically after an LLM or Agent node. This captures the response and writes it back to your dataset in Google Sheets.\n3. To evaluate output quality, add a third **Evaluation** node with the **Set Metrics** operation at a point after youâ€™ve generated the outputs. You can develop workflow logic, custom calculations, or add an LLM-as-Judge to score the outputs. Map these metrics to your dataset in the nodeâ€™s parameters. <br>\n\n    > ðŸ’¡ Well-defined metrics = smarter decisions. Scoring your outputs based on similarity, correctness, or categorization can help you track whether changes are actually improving performance. Learn more and get links"
}