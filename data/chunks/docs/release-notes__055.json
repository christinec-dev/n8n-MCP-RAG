{
  "source": "docs/release-notes.md",
  "index": 55,
  "content": "response and writes it back to your dataset in Google Sheets.\n3. To evaluate output quality, add a third **Evaluation** node with the **Set Metrics** operation at a point after youâ€™ve generated the outputs. You can develop workflow logic, custom calculations, or add an LLM-as-Judge to score the outputs. Map these metrics to your dataset in the nodeâ€™s parameters. <br>\n\n    > ðŸ’¡ Well-defined metrics = smarter decisions. Scoring your outputs based on similarity, correctness, or categorization can help you track whether changes are actually improving performance. Learn more and get links to example templates [here](/advanced-ai/evaluations/metric-based-evaluations.md/#2-calculate-metrics). \n    \n<br>\n\n<figure markdown=\"span\">\n    ![Evaluation workflow](/_images/release-notes/Evaluations_workflow.png)\n    <figcaption>Evaluation workflow</figcaption>\n</figure>\n<br>\n\nWhen the Evaluation trigger node is executed, it runs each input in our dataset through your AI logic. This continues until all test cases are processed, a limit is reached, or you manually stop the execution. Once your evaluation path is set up, you can update your prompt, model, or workflow logicâ€”and re-run the Evaluation trigger node to compare results. If youâ€™ve added metrics, theyâ€™ll appear in the Evaluations tab. <br><br>\n\nIn some instances, you may want to isolate your testing path to make iteration faster or to avoid executing downstream logic.  In this case, you can add an Evaluation node with the `Check If Evaluating` operation to ensure only the expected nodes run when performing evaluations. <br><br>\n\n#### Things to keep in mind\n\nEvaluations for AI Workflows are designed to fit  into your development flow, with more enhancements on the way. For now, here are a few things to note:\n\n- Test datasets are currently managed through Google Sheets. Youâ€™ll need a Google Sheets credential to run evaluations.\n- Each workflow supports one evaluation at a time. If youâ€™d like to test multiple segments, consider splitting them into sub-workflows for more flexibility.\n- Community Edition supports one single evaluation. Pro and Enterprise plans allow unlimited evaluations.\n- AI Evaluations are not enabled for instances in scaling mode at this time. <br>\n\nYou can find details, tips, and common troubleshooting info [here](https://docs.n8n.io/advanced-ai/evaluations/tips-and-common-issues/). <br><br>\n\n ðŸ‘‰ Learn more about the AI evaluation strategies and practical implementation techniques. [Watch now](https://www.youtube.com/live/QkciQpotQBQ?feature=shared).\n\n</div>"
}