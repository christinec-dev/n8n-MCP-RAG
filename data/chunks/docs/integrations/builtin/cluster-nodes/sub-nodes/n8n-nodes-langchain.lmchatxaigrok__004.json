{
  "source": "docs/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatxaigrok.md",
  "index": 4,
  "content": "## Node options\n\n* **Frequency Penalty**: Use this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\n* **Maximum Number of Tokens**: Enter the maximum number of tokens used, which sets the completion length. Most models have a context length of 2048 tokens with the newest models supporting up to 32,768 tokens. \n* **Response Format**: Choose **Text** or **JSON**. **JSON** ensures the model returns valid JSON.\n* **Presence Penalty**: Use this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n* **Timeout**: Enter the maximum request time in milliseconds.\n* **Max Retries**: Enter the maximum number of times to retry a request.\n* **Top P**: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options."
}