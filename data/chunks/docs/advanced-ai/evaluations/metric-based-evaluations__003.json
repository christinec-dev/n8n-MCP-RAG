{
  "source": "docs/advanced-ai/evaluations/metric-based-evaluations.md",
  "index": 3,
  "content": "### What are metric-based evaluations?\n\nOnce your workflow is ready for deployment, you often want to test it on more examples than [when you were building it](/advanced-ai/evaluations/light-evaluations.md).\n\nFor example, when production executions start to turn up edge cases, you want to add them to your test dataset so that you can make sure they're covered.\n\nFor large datasets like the ones built from production data, it can be hard to get a sense of performance just by eyeballing the results. Instead, you must measure performance. Metric-based evaluations can assign one or more scores to each test run, which you can compare to previous runs. Individual scores get rolled up to measure performance on the whole dataset. \n\nThis feature allows you to run evaluations that calculate metrics, track how those metrics change between runs and drill down into the reasons for those changes.\n\nMetrics can be deterministic functions (such as the distance between two strings) or you can calculate them using AI. Metrics often involve checking how far away the output is from a *reference output* (also called ground truth). To do so, the dataset must contain that reference output. Some evaluations don't need this reference output though (for example, checking text for sentiment or toxicity)."
}