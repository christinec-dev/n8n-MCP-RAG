{
  "source": "docs/advanced-ai/evaluations/overview.md",
  "index": 3,
  "content": "## What are evaluations?\n\nEvaluation is a crucial technique for checking that your AI workflow is reliable. It can be the difference between a flaky proof of concept and a solid production workflow. It's important both in the building phase and after deploying to production. \n\nThe foundation of evaluation is running a test dataset through your workflow. This dataset contains multiple test cases. Each test case contains a sample input for your workflow, and often includes the expected output(s) too.\n\nEvaluation allows you to:\n\n* **Test your workflow over a range of inputs** so you know how it performs on edge cases\n* **Make changes with confidence** without inadvertently making things worse elsewhere\n* **Compare performance** across different models or prompts\n\nThe following video explains what evaluations are, why they're useful, and how they work:\n\n<div class=\"video-container\">\n<iframe width=\"840\" height=\"472.5\" src=\"https://www.youtube.com/embed/5LlF196PKaE\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n</div>"
}