{
  "source": "docs/advanced-ai/evaluations/metric-based-evaluations.md",
  "index": 6,
  "content": "### 2. Calculate metrics\n\nMetrics are dimensions used to score the output of your workflow. They often compare the actual workflow output with a reference output. It's common to use AI to calculate metrics, although it's sometimes possible to just use code. In n8n, metrics are always numbers.\n\nYou need to add the logic to calculate the metrics for your workflow, at a point after it has produced the outputs. You can add any reference outputs your metric uses as a column in your dataset. This makes sure they it will be available in the workflow, since they will be output by the evaluation trigger.\n\nExamples:\n\n* [Correctness](https://n8n.io/workflows/4271): whether the output's meaning is consistent with a reference output.\n* [Categorization](https://n8n.io/workflows/4269): whether the output exactly matches the expected output.\n* Helpfulness: whether the answer addresses the question.\n* [String similarity](https://n8n.io/workflows/4274): how close the output is to a reference output, measured character-by-character.\n* [Tool calling](https://n8n.io/workflows/4268): whether the agent called the right tool.\n* [RAG document relevance](https://n8n.io/workflows/4273): when working with a vector database, whether the documents retrieved are relevant to the question.\n* RAG answer groundedness: when working with a vector database, whether the answer is [\"grounded\"](https://www.deepset.ai/blog/rag-llm-evaluation-groundedness) in the documents retrieved.\n\nCalculating metrics can add latency and cost, so you may only want to do it when running an evaluation and avoid it when making a production execution. You can do this by putting the metric logic after a ['check if evaluating' operation](/integrations/builtin/core-nodes/n8n-nodes-base.evaluation.md#check-if-evaluating).\n\n![Check if evaluating node](/_images/advanced-ai/evaluations/check-if-evaluating.png)"
}