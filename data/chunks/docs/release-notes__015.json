{
  "source": "docs/release-notes.md",
  "index": 15,
  "content": "### Built-in Metrics for AI Evaluations\n\nUsing evaluations is a best practice for any AI solution, and a must if reliability and predictability are business-critical. With this release, we‚Äôve made it easier to set up evaluations in n8n by introducing a set of built-in metrics. These metrics can review AI responses and assign scores based on factors like correctness, helpfulness, and more.<br><br>\n\n\nYou can run regular evaluations and review scores over time as a way to monitor your AI workflow's performance. You can also compare results across different models to help guide model selection, or run evaluations before and after a prompt change to support data-driven, iterative building.\n<br><br>\nAs with all evaluations in n8n, you‚Äôll need a dataset that includes the inputs you want to test. For some evaluations, the dataset must also include expected outputs (ground truth) to compare against. The evaluation workflow runs each input through the portion you're testing to generate a response. The built-in metric scores each response based on the aspect you're measuring, allowing you to compare results before and after changes or track trends over time in the Evaluations tab. <br><br>\n\nYou can still define your own custom metrics, but for common use cases, the built-in options make it much faster to implement. <br><br>\n\nüõ†Ô∏è **How to:** \n\n1. Set up your evaluation as described [here](https://docs.n8n.io/advanced-ai/evaluations/metric-based-evaluations/#how-it-works), using an **Evaluation** node as the trigger and another with the **Set Metrics** operation.\n2. In the **Set Metrics** node, choose a metric from the dropdown list.\n3. Define any additional parameters required for your selected metric. In most cases, this includes mapping the dataset columns to the appropriate fields.\n\nüìè **Available built-in metrics:**\n\n- **Correctness (AI-based):** Compares AI workflow-generated responses to expected answers. Another LLM acts as a judge, scoring the responses based on guidance you provide in the prompt.\n- **Helpfulness (AI-based):** Evaluates how helpful a response is in relation to a user query, using an LLM and prompt-defined scoring criteria.\n- **String Similarity:** Measures how closely the response matches the expected output by comparing strings. Useful for command generation or when output needs to follow a specific structure.\n- **Categorization:** Checks whether a response matches an expected label, such as assigning items to the correct category.\n- **Tools Used:** Verifies whether the AI agent called the tools you specified in your dataset. \nTo enable this, make sure **Return Intermediate Steps** is turned on in your agent so the evaluation can access the tools it actually called.\n\nüß† Keep in mind\n\n- Registered Community Edition enables analysis of one evaluation in the¬†**Evaluations**¬†tab which allows easy comparison of evaluation runs over time. Pro and Enterprise plans allow unlimited evaluations in the¬†**Evaluations**¬†tab.\n\n<br>\n<figure markdown=\"span\">\n    ![Built-in Metrics](/_images/release-notes/Built-in_metrics.png)\n    <figcaption>Built-in Metrics</figcaption>\n</figure>\n<br>\n\n[Learn more](/advanced-ai/evaluations/overview.md) about setting up and customizing evaluations.\n</div>"
}